{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c41d5d-fb5d-42b5-9d75-a97dd8eaa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch-generates NL2SQL 结果并保存为 JSON 文件。\n",
    "\n",
    "关键优化：\n",
    "1. **一次读取模板 & format_map**避免在循环里多次 I/O 和正则替换。\n",
    "2. **批量推理**利用 pipeline 的 `batch_size` 提升吞吐。\n",
    "3. **no_grad + fp16/8bit**减小显存占用，推理更快。\n",
    "4. **路径与超参集中管理**便于脚本化 / 日后复现。\n",
    "5. **进度可视化 (tqdm)**长任务更直观。\n",
    "6. **异常兜底**避免单条数据崩溃拖垮整体。\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# ---------- 可调参数 ----------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = r\"/data/qwen2-7b-instruct\"\n",
    "    prompt_file: str = \"QP_prompt.txt\"\n",
    "    input_json: str = \"schema_linking_result.json\"\n",
    "    output_dir: str = \"result\"\n",
    "    max_new_tokens: int = 1536\n",
    "    batch_size: int = 4          # 根据显存灵活调整\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.6\n",
    "    use_fp16: bool = True        # 或用 8bit/4bit 量化\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "def load_prompt_template(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def format_prompt(template: str, db_id: str, question: str,\n",
    "                  evidence: str, schema_linking: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    使用 str.format_map 直接填充占位符，保持模板可读性。\n",
    "    模板里写 {db_id}、{question}、{evidence}、{schema_linking}\n",
    "    \"\"\"\n",
    "    return template.format_map({\n",
    "        \"db_id\": db_id,\n",
    "        \"question\": question,\n",
    "        \"evidence\": evidence,\n",
    "        \"schema_linking\": json.dumps(schema_linking, ensure_ascii=False),\n",
    "    })\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(cfg: Config):\n",
    "    # 量化示例：8bit\n",
    "    quant_cfg = None\n",
    "    if not cfg.use_fp16:\n",
    "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\",  # 对于 text-generation 更稳妥\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if cfg.use_fp16 else torch.float32,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=cfg.device_map,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batched(iterable: List[Any], n: int):\n",
    "    \"\"\"将列表分批切片 (Python 3.12 里有 itertools.batched)\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_sql_block(generated_text: str) -> str:\n",
    "    \"\"\"从模型输出中提取 ```sql ... ``` 中间内容\"\"\"\n",
    "    pattern = r\"```sql\\s+(.*?)\\s*```\"\n",
    "    matches = re.findall(pattern, generated_text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return generated_text.strip()  # fallback\n",
    "    \n",
    "def generate():\n",
    "    cfg = CFG\n",
    "    root = Path(__file__).resolve().parent\n",
    "    input_path = root / cfg.input_json\n",
    "    out_dir = root / cfg.output_dir\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    data: List[Dict[str, Any]] = json.loads(Path(input_path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    prompt_tpl = load_prompt_template(root / cfg.prompt_file)\n",
    "    tokenizer, model = load_model_and_tokenizer(cfg)\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=cfg.batch_size,\n",
    "        do_sample=cfg.do_sample,\n",
    "        device_map=cfg.device_map,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # —— 主循环 ——\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(list(batched(data, cfg.batch_size)), desc=\"Generating\"):\n",
    "            prompts = [\n",
    "                format_prompt(\n",
    "                    prompt_tpl,\n",
    "                    item.get(\"db_id\", \"\"),\n",
    "                    item.get(\"question\", \"\"),\n",
    "                    item.get(\"evidence\", \"\"),\n",
    "                    item.get(\"schema_linking\", {}),\n",
    "                )\n",
    "                for item in batch\n",
    "            ]\n",
    "\n",
    "            # 生成\n",
    "            outputs = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "            # pipeline 在 batch 模式下返回 List[List[Dict]]\n",
    "            for item, gen in zip(batch, outputs):\n",
    "                text: str = gen[0][\"generated_text\"]\n",
    "                sql_key = \"sql statement:\"\n",
    "                sql_start = text.lower().find(sql_key)\n",
    "                sql = text[sql_start + len(sql_key):].strip() if sql_start != -1 else text.strip()\n",
    "\n",
    "                result = {\n",
    "                    \"db_id\": item.get(\"db_id\"),\n",
    "                    \"question\": item.get(\"question\"),\n",
    "                    \"evidence\": item.get(\"evidence\"),\n",
    "                    \"schema_linking\": item.get(\"schema_linking\"),\n",
    "                    \"sql\": sql,\n",
    "                }\n",
    "                results.append(result)\n",
    "    return results\n",
    "\n",
    "cfg = CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d53f370-de68-49cd-99e1-c6c7bb693bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = Path(\"/home/yangliu26/CHASE/candidates\")\n",
    "input_path = root / cfg.input_json\n",
    "out_dir = root / cfg.output_dir\n",
    "out_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd66690-bbe3-4126-8e7f-ea294abf9849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data: List[Dict[str, Any]] = json.loads(Path(input_path).read_text(encoding=\"utf-8\"))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01306cf6-8216-4566-b82e-92da4694289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tpl = load_prompt_template(root / cfg.prompt_file)\n",
    "# print(format_prompt(\n",
    "#                 prompt_tpl,\n",
    "#                 \"test\",\n",
    "#                 \"hello, world\",\n",
    "#                 \"this is a hint\",\n",
    "#                 \"schema\",\n",
    "#             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff25724-e1c0-4262-ba25-d82105c9e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model_and_tokenizer(cfg)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=cfg.batch_size,\n",
    "    do_sample=cfg.do_sample,\n",
    "    temperature=cfg.temperature,\n",
    "    device_map=cfg.device_map,\n",
    "    return_full_text=False,     # 仅返回新增 token，不含 prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef0e2a6-60e6-4558-ae7a-7d42a113b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in translating natural language questions into SQL queries using a **Query Plan approach**.\n",
      "\n",
      "Given the following information:\n",
      "\n",
      "- **Database**: movie_platform\n",
      "- **Question**: What is the name of the longest movie title? When was it released?\n",
      "- **Evidence** (schema and sample data): longest movie title refers to MAX(LENGTH(movie_title)); when it was released refers to movie_release_year;\n",
      "- **Schema Linking Results**: {\"movies\": [\"movies\", \"movie_title\", \"movie_url\", \"movie_title\", \"movie_title_language\", \"movies\", \"movie_url\", \"movie_release_year\", \"director_name\", \"movies\", \"movie_title\", \"movie_id\", \"movie_release_year\", \"movie_title\", \"movies\", \"movie_id\", \"movies\", \"movie_release_year\"], \"ratings\": [\"ratings\", \"rating_url\", \"movie_id\", \"movie_id\", \"ratings\"], \"lists\": [\"list_creation_timestamp_utc\", \"list_update_timestamp_utc\"]}\n",
      "\n",
      "Your task is to perform the following:\n",
      "\n",
      "---\n",
      "1. Step-by-step, reason through the query by creating a **detailed Query Plan**.\n",
      "2. Then, based on the Query Plan, **write the final SQL query**.\n",
      "\n",
      "---\n",
      "\n",
      "Rules:\n",
      "\n",
      "- Do not skip any steps in the Query Plan. Think carefully and completely.\n",
      "- Do not output any additional text beyond the required sections.\n",
      "- Format your output exactly as shown above.\n",
      "- Always wrap the final SQL inside a ```sql code block.\n",
      "- Output both the Query Plan and Final SQL as required.\n",
      "\n",
      "---\n",
      "**Output Format**:\n",
      "\n",
      "### Query Plan:\n",
      "(Write a step-by-step reasoning for constructing the SQL query here.)\n",
      "\n",
      "### Final SQL:\n",
      "```sql\n",
      "(Write the final SQL query here.)\n",
      "```\n",
      "----\n",
      "\n",
      "## Reference structure for Query Plan:\n",
      "**Query Plan (step-by-step reasoning):**\n",
      "\n",
      "1. **Understand the intent**: Summarize what the user wants to retrieve or compute.\n",
      "2. **Locate target tables and columns**: Based on schema linking, determine which tables and columns are involved.\n",
      "3. **Identify filter conditions**: List the constraints (e.g., WHERE clauses) needed.\n",
      "4. **Determine aggregation, grouping, or ordering**: Specify if any aggregation functions (SUM, COUNT, AVG) are needed; check if GROUP BY or ORDER BY is needed.\n",
      "5. **Handle joins if multiple tables are needed**: Specify JOIN conditions if needed.\n",
      "6. **Build subqueries if required**: If the query requires nested logic, outline the subquery structure.\n",
      "7. **Formulate the final SQL query**: Write the SQL statement based on the plan.\n",
      "\n",
      "---\n",
      "\n",
      "Start now. Output only the Query Plan and the Final SQL exactly as instructed.\n"
     ]
    }
   ],
   "source": [
    "item = data[2]\n",
    "prompts = format_prompt(\n",
    "                prompt_tpl,\n",
    "                item.get(\"db_id\", \"\"),\n",
    "                item.get(\"question\", \"\"),\n",
    "                item.get(\"evidence\", \"\"),\n",
    "                item.get(\"schema_linking\", {}),\n",
    "            )\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfef90-b280-4e39-b9c8-aa3c63f0d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcaabf8-109a-43db-8dab-fed4e28c67c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1568836-0b0e-48aa-810c-81948d8b61eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" No additional text or explanations beyond these guidelines.\\n### Query Plan:\\n1. **Understand the intent**: The user wants to find the longest movie title and its release year.\\n2. **Locate target tables and columns**: The relevant table is'movies' with columns'movie_title' and'movie_release_year'.\\n3. **Identify filter conditions**: No specific filters are mentioned, so we'll use all rows from the'movies' table.\\n4. **Determine aggregation, grouping, or ordering**: We need to find the maximum length of movie titles, so we'll use MAX() function on'movie_title'. Also, order the results by this maximum length in descending order to easily identify the longest title.\\n5. **Handle joins if multiple tables are needed**: No joins are necessary since we're only working with the'movies' table.\\n6. **Build subqueries if required**: No subqueries are needed for this query.\\n7. **Formulate the final SQL query**: SELECT the maximum length of movie titles, the corresponding movie title, and the release year.\\n\\n### Final SQL:\\n```sql\\nSELECT \\n    movie_title,\\n    movie_release_year\\nFROM \\n    movies\\nWHERE \\n    LENGTH(movie_title) = (\\n        SELECT \\n            MAX(LENGTH(movie_title)) \\n        FROM \\n            movies\\n    )\\nORDER BY \\n    LENGTH(movie_title) DESC;\\n```\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0875efa3-7efe-4aab-a0cf-ffd59bb4c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    movie_title,\n",
      "    movie_release_year\n",
      "FROM \n",
      "    movies\n",
      "WHERE \n",
      "    LENGTH(movie_title) = (\n",
      "        SELECT \n",
      "            MAX(LENGTH(movie_title)) \n",
      "        FROM \n",
      "            movies\n",
      "    )\n",
      "ORDER BY \n",
      "    LENGTH(movie_title) DESC;\n"
     ]
    }
   ],
   "source": [
    "print(extract_sql_block(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e50f2be-5f02-4c82-9dc5-422e818e98af",
   "metadata": {},
   "source": [
    "# —— 主循环 ——\n",
    "cnt=0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(list(batched(data, cfg.batch_size)), desc=\"Generating\"):\n",
    "        prompts = [\n",
    "            format_prompt(\n",
    "                prompt_tpl,\n",
    "                item.get(\"db_id\", \"\"),\n",
    "                item.get(\"question\", \"\"),\n",
    "                item.get(\"evidence\", \"\"),\n",
    "                item.get(\"schema_linking\", {}),\n",
    "            )\n",
    "            for item in batch\n",
    "        ]\n",
    "        # print(prompts)\n",
    "        # 生成\n",
    "        outputs = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "        # pipeline 在 batch 模式下返回 List[List[Dict]]\n",
    "        for item, gen in zip(batch, outputs):\n",
    "            text: str = gen[0][\"generated_text\"]\n",
    "            sql_key = \"sql statement:\"\n",
    "            sql_start = text.lower().find(sql_key)\n",
    "            sql = text[sql_start + len(sql_key) :].strip() if sql_start != -1 else text.strip()\n",
    "\n",
    "            result = {\n",
    "                \"db_id\": item.get(\"db_id\"),\n",
    "                \"question\": item.get(\"question\"),\n",
    "                \"evidence\": item.get(\"evidence\"),\n",
    "                \"schema_linking\": item.get(\"schema_linking\"),\n",
    "                \"sql\": sql,\n",
    "            }\n",
    "\n",
    "            out_file = out_dir / f\"{item['db_id']}_{cnt}.json\"\n",
    "            cnt += 1\n",
    "            out_file.write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ All done! Results saved to: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ceb6b-0e72-4d67-b0c1-52871f02fb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
