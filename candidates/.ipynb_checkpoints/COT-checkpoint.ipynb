{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34273fa6-f604-4f33-951e-80ce4f825897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Divide-and-Conquer CoT 方法实现\n",
    "- 将复杂自然语言问题分解为多个子问题\n",
    "- 对每个子问题分别生成SQL片段\n",
    "- 最后合并这些片段，构造完整的SQL\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# ---------- 可调参数 ----------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = r\"/home/yangliu26/qwen3-8b\"  # 请根据实际模型路径调整\n",
    "    input_json: str = r\"/home/yangliu26/CHASE/schema_linking_lxy/schema_linking_result.json\"\n",
    "    output_dir: str = r\"/home/yangliu26/CHASE/candidates/cot_result\"\n",
    "    # 文本生成超参\n",
    "    max_new_tokens: int = 1024\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.2\n",
    "    # 性能设置\n",
    "    batch_size: int = 4\n",
    "    use_fp16: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "# 配置实例\n",
    "CFG = Config()\n",
    "\n",
    "def load_model_and_tokenizer(cfg: Config):\n",
    "    \"\"\"加载模型和分词器\"\"\"\n",
    "    # 量化配置\n",
    "    quant_cfg = None\n",
    "    if not cfg.use_fp16:\n",
    "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\",\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if cfg.use_fp16 else torch.float32,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=cfg.device_map,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "def batched(iterable: List[Any], n: int):\n",
    "    \"\"\"将列表分批切片\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "def load_prompt_template(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def decompose_question(question: str, db_schema: str, generator) -> List[str]:\n",
    "    # 分解步骤的提示词模板\n",
    "    DECOMPOSE_TEMPLATE = load_prompt_template(r\"/home/yangliu26/CHASE/template/decompose_template.txt\")\n",
    "\n",
    "    \"\"\"将问题分解为多个子问题\"\"\"\n",
    "    prompt = DECOMPOSE_TEMPLATE.format(\n",
    "        question=question,\n",
    "        db_schema=db_schema\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "    text = response[0][\"generated_text\"]\n",
    "    \n",
    "    # 解析子问题\n",
    "    sub_questions = []\n",
    "    for line in text.strip().split('\\n'):\n",
    "        if line.strip() and any(line.strip().startswith(str(i)) for i in range(1, 10)):\n",
    "            # 移除序号前缀\n",
    "            sub_q = line.strip()\n",
    "            for i in range(1, 10):\n",
    "                prefix = f\"{i}. \"\n",
    "                if sub_q.startswith(prefix):\n",
    "                    sub_q = sub_q[len(prefix):]\n",
    "                    break\n",
    "            sub_questions.append(sub_q)\n",
    "    \n",
    "    return sub_questions\n",
    "\n",
    "def generate_partial_sql(sub_question: str, db_schema: str, generator) -> str:\n",
    "     # 生成SQL片段的提示词模板\n",
    "    PARTIAL_SQL_TEMPLATE = load_prompt_template(r\"CHASE_work\\template\\partial_sql_template.txt\")\n",
    "    \n",
    "    \"\"\"为子问题生成SQL片段\"\"\"\n",
    "    prompt = PARTIAL_SQL_TEMPLATE.format(\n",
    "        sub_question=sub_question,\n",
    "        db_schema=db_schema\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "def assemble_sql(question: str, db_schema: str, sub_questions: List[str], \n",
    "                partial_sqls: List[str], generator) -> str:\n",
    "    # 组合SQL的提示词模板\n",
    "    ASSEMBLE_TEMPLATE = load_prompt_template(r\"CHASE_work\\template\\assemble_template.txt\")\n",
    "    \n",
    "    \"\"\"组合SQL片段为完整SQL\"\"\"\n",
    "    # 格式化子问题和SQL片段\n",
    "    sub_qs_and_sqls = \"\"\n",
    "    for i, (q, sql) in enumerate(zip(sub_questions, partial_sqls), 1):\n",
    "        sub_qs_and_sqls += f\"{i}. 子问题: {q}\\n   SQL片段: {sql}\\n\\n\"\n",
    "    \n",
    "    prompt = ASSEMBLE_TEMPLATE.format(\n",
    "        question=question,\n",
    "        db_schema=db_schema,\n",
    "        sub_questions_and_sqls=sub_qs_and_sqls\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=1024, do_sample=False)\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "def optimize_sql(sql: str, generator) -> str:\n",
    "    \"\"\"优化SQL查询（可选）\"\"\"\n",
    "    # 这里可以添加SQL优化逻辑，如去除冗余等\n",
    "    # 简单实现，直接返回\n",
    "    return sql\n",
    "\n",
    "def divide_and_conquer_sql(question: str, db_schema: str, generator):\n",
    "    \"\"\"主函数：使用分而治之方法生成SQL\"\"\"\n",
    "    # 步骤1: 分解问题\n",
    "    sub_questions = decompose_question(question, db_schema, generator)\n",
    "    \n",
    "    # 步骤2: 生成每个子问题的SQL片段\n",
    "    partial_sqls = []\n",
    "    for q in sub_questions:\n",
    "        partial_sql = generate_partial_sql(q, db_schema, generator)\n",
    "        partial_sqls.append(partial_sql)\n",
    "    \n",
    "    # 步骤3: 汇总构造最终SQL\n",
    "    final_sql = assemble_sql(question, db_schema, sub_questions, partial_sqls, generator)\n",
    "    \n",
    "    return optimize_sql(final_sql, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d42165-d6b9-4bb5-975d-de7a762139ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"处理数据并生成SQL\"\"\"\n",
    "# 创建输出目录(若已存在,则不用创建)\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "\n",
    "# 加载数据\n",
    "with open(CFG.input_json, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdffff59-8898-443e-8d37-6f2639488468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ec882899364ae98e147363d2fa0c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer, model = load_model_and_tokenizer(CFG)\n",
    "\n",
    "# 创建生成器\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6991577b-2a84-444d-a5b6-e7f39bba64dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'movie_platform',\n",
       " 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       " 'keywords': ['movie titles', 'released', '1945', 'descending', 'popularity'],\n",
       " 'schema_linking': {'movies': ['movie_title',\n",
       "   'movies',\n",
       "   'movie_title_language',\n",
       "   'movie_release_year',\n",
       "   'movie_popularity',\n",
       "   'movie_release_year',\n",
       "   'movie_title',\n",
       "   'movies',\n",
       "   'movie_id',\n",
       "   'movie_release_year',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies'],\n",
       "  'ratings': ['movie_id',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic_likes',\n",
       "   'rating_url'],\n",
       "  'lists': ['lists']}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item=data[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e2e4e6-ee64-48c9-a685-9e4d9560c204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " {'movies': ['movie_title',\n",
       "   'movies',\n",
       "   'movie_title_language',\n",
       "   'movie_release_year',\n",
       "   'movie_popularity',\n",
       "   'movie_release_year',\n",
       "   'movie_title',\n",
       "   'movies',\n",
       "   'movie_id',\n",
       "   'movie_release_year',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies'],\n",
       "  'ratings': ['movie_id',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic_likes',\n",
       "   'rating_url'],\n",
       "  'lists': ['lists']},\n",
       " 'movie_platform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = item.get(\"question\", \"\")\n",
    "db_schema = item.get(\"schema_linking\", \"\")  # 假设数据中有db_schema字段\n",
    "db_id = item.get(\"db_id\", \"\")\n",
    "question, db_schema, db_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39f5eca-b80a-45b7-85b4-94141daf2d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional database expert. Your task is to decompose a complex natural language question into multiple simpler sub-questions.\n",
      "\n",
      "**Database Information:**  \n",
      "{db_schema}\n",
      "\n",
      "**Original Question:**  \n",
      "{question}\n",
      "\n",
      "Please break down the question into 2 to 4 sub-questions. Each sub-question should be simpler and easier to convert into SQL fragments.  \n",
      "\n",
      "**Output Format:**  \n",
      "1. Sub-question 1  \n",
      "2. Sub-question 2  \n",
      "...\n",
      "\n",
      "Only output the list of sub-questions. Do not include any explanations.\n"
     ]
    }
   ],
   "source": [
    "# 分解步骤的提示词模板\n",
    "DECOMPOSE_TEMPLATE = load_prompt_template(r\"/home/yangliu26/CHASE/template/decompose_template.txt\")\n",
    "print(DECOMPOSE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55ff6fdc-5587-4aab-bf10-a6c7b018f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional database expert. Your task is to decompose a complex natural language question into multiple simpler sub-questions.\n",
      "\n",
      "**Database Information:**  \n",
      "{'movies': ['movie_title', 'movies', 'movie_title_language', 'movie_release_year', 'movie_popularity', 'movie_release_year', 'movie_title', 'movies', 'movie_id', 'movie_release_year', 'movies', 'movie_title', 'movie_popularity', 'movies', 'movie_title', 'movie_popularity', 'movies'], 'ratings': ['movie_id', 'critic', 'ratings', 'critic', 'ratings', 'critic_likes', 'rating_url'], 'lists': ['lists']}\n",
      "\n",
      "**Original Question:**  \n",
      "Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\n",
      "\n",
      "Please break down the question into 2 to 4 sub-questions. Each sub-question should be simpler and easier to convert into SQL fragments.  \n",
      "\n",
      "**Output Format:**  \n",
      "1. Sub-question 1  \n",
      "2. Sub-question 2  \n",
      "...\n",
      "\n",
      "Only output the list of sub-questions. Do not include any explanations.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"将问题分解为多个子问题\"\"\"\n",
    "prompt = DECOMPOSE_TEMPLATE.format(\n",
    "    question=question,\n",
    "    db_schema=db_schema\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b598b985-e948-4166-9bd0-1993dc799f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Do not use markdown. Do not include the original question. Do not include the database schema.\n",
      "Okay, let's see. The user wants movie titles from 1945 sorted by popularity descending. First, I need to find all movies released in 1945. But looking at the database schema, the'movies' table has multiple entries with columns like movie_title, movie_release_year, and movie_popularity. Wait, the schema seems a bit messy with repeated column names. Maybe the actual columns are movie_title, movie_release_year, and movie_popularity? The'movies' table has entries like'movie_title','movie_release_year', etc. So the first step is to get all movie titles where the release year is 1945.\n",
      "\n",
      "Then, the second part is sorting them by popularity in descending order. But how are the popularity values stored? The'movies' table has a'movie_popularity' column. So the sub-questions would be: first, select the movie titles where the release year is 1945. Second, sort those titles by popularity in descending order. Wait, but the original question mentions the'movies' table has multiple instances of the same columns. Maybe the actual table has unique columns. Let me recheck the schema.\n",
      "\n",
      "The'movies' list includes'movie_title','movie_release_year','movie_popularity', etc. So the first sub-question is to filter movies with release year 1945. The second is to sort those by popularity descending. But the user might need to ensure that the movie titles are unique, but the schema doesn't mention a primary key. However, the original question just asks for the titles, so maybe duplicates are allowed. So the two sub-questions would be: 1. Retrieve all movie titles from the movies table where the release year is 1945. 2. Sort the retrieved movie titles by movie popularity in descending order. That's two sub-questions. Wait, but the original question might require joining with another table? Wait, the 'ratings' table has'movie_id' and 'ratings', but the question is about movie titles and release year, so maybe it's only the'movies' table. So the two sub-questions are sufficient. Alternatively, maybe the popularity is in the 'ratings' table? But the original question says to sort by movie popularity, which is likely in the'movies' table. So the two sub-questions are correct.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "text = response[0][\"generated_text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1cf55ec-f6d2-4680-9e7e-396e91cb0f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解析子问题\n",
    "sub_questions = []\n",
    "for line in text.strip().split('\\n'):\n",
    "    if line.strip() and any(line.strip().startswith(str(i)) for i in range(1, 10)):\n",
    "        # 移除序号前缀\n",
    "        sub_q = line.strip()\n",
    "        for i in range(1, 10):\n",
    "            prefix = f\"{i}. \"\n",
    "            if sub_q.startswith(prefix):\n",
    "                sub_q = sub_q[len(prefix):]\n",
    "                break\n",
    "        sub_questions.append(sub_q)\n",
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f850eb-9611-4685-945e-8ab93704bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理每个样本\n",
    "results = []\n",
    "for i, item in enumerate(tqdm(data, desc=\"Processing\")):\n",
    "    try:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        db_schema = item.get(\"db_schema\", \"\")  # 假设数据中有db_schema字段\n",
    "        db_id = item.get(\"db_id\", f\"db_{i}\")\n",
    "        \n",
    "        # 使用Divide-and-Conquer方法生成SQL\n",
    "        sql = divide_and_conquer_sql(question, db_schema, generator)\n",
    "        \n",
    "        # 保存结果\n",
    "        result = {\n",
    "            \"db_id\": db_id,\n",
    "            \"question\": question,\n",
    "            \"sql\": sql,\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # 写入单个文件\n",
    "        output_file = os.path.join(CFG.output_dir, f\"{db_id}_{i}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {i}: {e}\")\n",
    "\n",
    "# 写入汇总文件\n",
    "with open(os.path.join(CFG.output_dir, \"all_results.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"处理完成，结果保存在: {CFG.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
