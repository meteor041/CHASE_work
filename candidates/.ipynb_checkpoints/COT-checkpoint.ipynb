{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34273fa6-f604-4f33-951e-80ce4f825897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Divide-and-Conquer CoT 方法实现\n",
    "- 将复杂自然语言问题分解为多个子问题\n",
    "- 对每个子问题分别生成SQL片段\n",
    "- 最后合并这些片段，构造完整的SQL\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# ---------- 可调参数 ----------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = r\"/home/yangliu26/qwen3-8b\"  # 请根据实际模型路径调整\n",
    "    input_json: str = r\"/home/yangliu26/CHASE/schema_linking_lxy/schema_linking_result.json\"\n",
    "    output_dir: str = r\"/home/yangliu26/CHASE/candidates/cot_result\"\n",
    "    # 文本生成超参\n",
    "    max_new_tokens: int = 1024\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.2\n",
    "    # 性能设置\n",
    "    batch_size: int = 4\n",
    "    use_fp16: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "# 配置实例\n",
    "CFG = Config()\n",
    "\n",
    "def load_model_and_tokenizer(cfg: Config):\n",
    "    \"\"\"加载模型和分词器\"\"\"\n",
    "    # 量化配置\n",
    "    quant_cfg = None\n",
    "    if not cfg.use_fp16:\n",
    "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\",\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if cfg.use_fp16 else torch.float32,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=cfg.device_map,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "def batched(iterable: List[Any], n: int):\n",
    "    \"\"\"将列表分批切片\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "def load_prompt_template(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def decompose_question(question: str, db_schema: str, generator) -> List[str]:\n",
    "    # 分解步骤的提示词模板\n",
    "    DECOMPOSE_TEMPLATE = load_prompt_template(r\"/home/yangliu26/CHASE/template/decompose_template.txt\")\n",
    "\n",
    "    \"\"\"将问题分解为多个子问题\"\"\"\n",
    "    prompt = DECOMPOSE_TEMPLATE.format(\n",
    "        question=question,\n",
    "        db_schema=db_schema\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "    text = response[0][\"generated_text\"]\n",
    "    \n",
    "    # 解析子问题\n",
    "    sub_questions = []\n",
    "    for line in text.strip().split('\\n'):\n",
    "        if line.strip() and any(line.strip().startswith(str(i)) for i in range(1, 10)):\n",
    "            # 移除序号前缀\n",
    "            sub_q = line.strip()\n",
    "            for i in range(1, 10):\n",
    "                prefix = f\"{i}. \"\n",
    "                if sub_q.startswith(prefix):\n",
    "                    sub_q = sub_q[len(prefix):]\n",
    "                    break\n",
    "            sub_questions.append(sub_q)\n",
    "    \n",
    "    return sub_questions\n",
    "\n",
    "def generate_partial_sql(sub_question: str, db_schema: str, generator) -> str:\n",
    "     # 生成SQL片段的提示词模板\n",
    "    PARTIAL_SQL_TEMPLATE = load_prompt_template(r\"CHASE_work\\template\\partial_sql_template.txt\")\n",
    "    \n",
    "    \"\"\"为子问题生成SQL片段\"\"\"\n",
    "    prompt = PARTIAL_SQL_TEMPLATE.format(\n",
    "        sub_question=sub_question,\n",
    "        db_schema=db_schema\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "def assemble_sql(question: str, db_schema: str, sub_questions: List[str], \n",
    "                partial_sqls: List[str], generator) -> str:\n",
    "    # 组合SQL的提示词模板\n",
    "    ASSEMBLE_TEMPLATE = load_prompt_template(r\"CHASE_work\\template\\assemble_template.txt\")\n",
    "    \n",
    "    \"\"\"组合SQL片段为完整SQL\"\"\"\n",
    "    # 格式化子问题和SQL片段\n",
    "    sub_qs_and_sqls = \"\"\n",
    "    for i, (q, sql) in enumerate(zip(sub_questions, partial_sqls), 1):\n",
    "        sub_qs_and_sqls += f\"{i}. 子问题: {q}\\n   SQL片段: {sql}\\n\\n\"\n",
    "    \n",
    "    prompt = ASSEMBLE_TEMPLATE.format(\n",
    "        question=question,\n",
    "        db_schema=db_schema,\n",
    "        sub_questions_and_sqls=sub_qs_and_sqls\n",
    "    )\n",
    "    \n",
    "    response = generator(prompt, max_new_tokens=1024, do_sample=False)\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "def optimize_sql(sql: str, generator) -> str:\n",
    "    \"\"\"优化SQL查询（可选）\"\"\"\n",
    "    # 这里可以添加SQL优化逻辑，如去除冗余等\n",
    "    # 简单实现，直接返回\n",
    "    return sql\n",
    "\n",
    "def divide_and_conquer_sql(question: str, db_schema: str, generator):\n",
    "    \"\"\"主函数：使用分而治之方法生成SQL\"\"\"\n",
    "    # 步骤1: 分解问题\n",
    "    sub_questions = decompose_question(question, db_schema, generator)\n",
    "    \n",
    "    # 步骤2: 生成每个子问题的SQL片段\n",
    "    partial_sqls = []\n",
    "    for q in sub_questions:\n",
    "        partial_sql = generate_partial_sql(q, db_schema, generator)\n",
    "        partial_sqls.append(partial_sql)\n",
    "    \n",
    "    # 步骤3: 汇总构造最终SQL\n",
    "    final_sql = assemble_sql(question, db_schema, sub_questions, partial_sqls, generator)\n",
    "    \n",
    "    return optimize_sql(final_sql, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d42165-d6b9-4bb5-975d-de7a762139ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"处理数据并生成SQL\"\"\"\n",
    "# 创建输出目录(若已存在,则不用创建)\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "\n",
    "# 加载数据\n",
    "with open(CFG.input_json, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdffff59-8898-443e-8d37-6f2639488468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1422d15ab2d04f8cb29f3f8fc56231f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载模型和分词器\n",
    "tokenizer, model = load_model_and_tokenizer(CFG)\n",
    "\n",
    "# 创建生成器\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6991577b-2a84-444d-a5b6-e7f39bba64dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'movie_platform',\n",
       " 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;',\n",
       " 'keywords': ['movie titles', 'released', '1945', 'descending', 'popularity'],\n",
       " 'schema_linking': {'movies': ['movie_title',\n",
       "   'movies',\n",
       "   'movie_title_language',\n",
       "   'movie_release_year',\n",
       "   'movie_popularity',\n",
       "   'movie_release_year',\n",
       "   'movie_title',\n",
       "   'movies',\n",
       "   'movie_id',\n",
       "   'movie_release_year',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies'],\n",
       "  'ratings': ['movie_id',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic_likes',\n",
       "   'rating_url'],\n",
       "  'lists': ['lists']}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item=data[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e2e4e6-ee64-48c9-a685-9e4d9560c204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.',\n",
       " {'movies': ['movie_title',\n",
       "   'movies',\n",
       "   'movie_title_language',\n",
       "   'movie_release_year',\n",
       "   'movie_popularity',\n",
       "   'movie_release_year',\n",
       "   'movie_title',\n",
       "   'movies',\n",
       "   'movie_id',\n",
       "   'movie_release_year',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies',\n",
       "   'movie_title',\n",
       "   'movie_popularity',\n",
       "   'movies'],\n",
       "  'ratings': ['movie_id',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic',\n",
       "   'ratings',\n",
       "   'critic_likes',\n",
       "   'rating_url'],\n",
       "  'lists': ['lists']},\n",
       " 'movie_platform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = item.get(\"question\", \"\")\n",
    "db_schema = item.get(\"schema_linking\", \"\")  # 假设数据中有db_schema字段\n",
    "db_id = item.get(\"db_id\", \"\")\n",
    "question, db_schema, db_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39f5eca-b80a-45b7-85b4-94141daf2d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional database expert. Your task is to decompose a complex natural language question into multiple simpler sub-questions.\n",
      "\n",
      "**Database Information:**  \n",
      "{db_schema}\n",
      "\n",
      "**Original Question:**  \n",
      "{question}\n",
      "\n",
      "Please break down the question into 2 to 4 sub-questions. Each sub-question should be simpler and easier to convert into SQL fragments.  \n",
      "\n",
      "**Output Format:**  \n",
      "1. Sub-question 1  \n",
      "2. Sub-question 2  \n",
      "...\n",
      "\n",
      "Only output the list of sub-questions. Do not include any explanations.\n"
     ]
    }
   ],
   "source": [
    "# 分解步骤的提示词模板\n",
    "DECOMPOSE_TEMPLATE = load_prompt_template(r\"/home/yangliu26/CHASE/template/decompose_template.txt\")\n",
    "print(DECOMPOSE_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55ff6fdc-5587-4aab-bf10-a6c7b018f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional database expert. Your task is to decompose a complex natural language question into multiple simpler sub-questions.\n",
      "\n",
      "**Database Information:**  \n",
      "{'movies': ['movie_title', 'movies', 'movie_title_language', 'movie_release_year', 'movie_popularity', 'movie_release_year', 'movie_title', 'movies', 'movie_id', 'movie_release_year', 'movies', 'movie_title', 'movie_popularity', 'movies', 'movie_title', 'movie_popularity', 'movies'], 'ratings': ['movie_id', 'critic', 'ratings', 'critic', 'ratings', 'critic_likes', 'rating_url'], 'lists': ['lists']}\n",
      "\n",
      "**Original Question:**  \n",
      "Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\n",
      "\n",
      "Please break down the question into 2 to 4 sub-questions. Each sub-question should be simpler and easier to convert into SQL fragments.  \n",
      "\n",
      "**Output Format:**  \n",
      "1. Sub-question 1  \n",
      "2. Sub-question 2  \n",
      "...\n",
      "\n",
      "Only output the list of sub-questions. Do not include any explanations.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"将问题分解为多个子问题\"\"\"\n",
    "prompt = DECOMPOSE_TEMPLATE.format(\n",
    "    question=question,\n",
    "    db_schema=db_schema\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b598b985-e948-4166-9bd0-1993dc799f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Only output the list of sub-question numbers.  \n",
      "\n",
      "1. Identify the movie titles released in the year 1945.  \n",
      "2. Determine the popularity of each identified movie from the movies table.  \n",
      "3. Sort the resulting list of movies by popularity in descending order.  \n",
      "4. Retrieve and list the movie titles in the sorted order.  \n",
      "5. Ensure that each movie title is unique and not repeated in the final list.  \n",
      "6. Verify that the movies are correctly associated with their respective popularity values.  \n",
      "7. Check for any possible duplicates in the movie titles and handle them appropriately.  \n",
      "8. Confirm that the sorting is done based on the correct popularity metric.  \n",
      "9. Validate that the final output includes only movies from the year 1945.  \n",
      "10. Ensure the output format is a simple list of movie titles sorted by popularity.  \n",
      "11. Confirm that the movie titles are correctly retrieved from the movies table.  \n",
      "12. Check if the popularity values are correctly linked to their respective movies.  \n",
      "13. Ensure that the sorting is in descending order as specified.  \n",
      "14. Verify that the final list is free of any extraneous data or errors.  \n",
      "15. Make sure that the query is optimized for performance and accuracy.  \n",
      "16. Confirm that the output is presented in the required format.  \n",
      "17. Ensure that the movie titles are correctly associated with the year 1945.  \n",
      "18. Validate that the popularity data is accurate and up-to-date.  \n",
      "19. Check for any inconsistencies in the data that might affect the results.  \n",
      "20. Ensure that the final list is sorted correctly and meets the user's requirements.  \n",
      "21. Confirm that the query retrieves only the necessary data.  \n",
      "22. Verify that the movie titles are correctly formatted and displayed.  \n",
      "23. Ensure that the popularity values are sorted in descending order.  \n",
      "24. Validate that the output includes all movies from 1945 with their correct popularity.  \n",
      "25. Confirm that the sorting is applied to the correct column.  \n",
      "26. Ensure that the final output is accurate and meets the user's specifications.  \n",
      "27. Check that the movie titles are correctly retrieved and sorted.  \n",
      "28. Verify that the popularity values are correctly used for sorting.  \n",
      "29. Ensure that the final list is properly formatted and easy to read.  \n",
      "30. Confirm that the query is correctly structured and efficient.  \n",
      "31\n"
     ]
    }
   ],
   "source": [
    "response = generator(prompt, max_new_tokens=512, do_sample=True)\n",
    "text = response[0][\"generated_text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1cf55ec-f6d2-4680-9e7e-396e91cb0f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解析子问题\n",
    "sub_questions = []\n",
    "for line in text.strip().split('\\n'):\n",
    "    if line.strip() and any(line.strip().startswith(str(i)) for i in range(1, 10)):\n",
    "        # 移除序号前缀\n",
    "        sub_q = line.strip()\n",
    "        for i in range(1, 10):\n",
    "            prefix = f\"{i}. \"\n",
    "            if sub_q.startswith(prefix):\n",
    "                sub_q = sub_q[len(prefix):]\n",
    "                break\n",
    "        sub_questions.append(sub_q)\n",
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f850eb-9611-4685-945e-8ab93704bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理每个样本\n",
    "results = []\n",
    "for i, item in enumerate(tqdm(data, desc=\"Processing\")):\n",
    "    try:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        db_schema = item.get(\"db_schema\", \"\")  # 假设数据中有db_schema字段\n",
    "        db_id = item.get(\"db_id\", f\"db_{i}\")\n",
    "        \n",
    "        # 使用Divide-and-Conquer方法生成SQL\n",
    "        sql = divide_and_conquer_sql(question, db_schema, generator)\n",
    "        \n",
    "        # 保存结果\n",
    "        result = {\n",
    "            \"db_id\": db_id,\n",
    "            \"question\": question,\n",
    "            \"sql\": sql,\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # 写入单个文件\n",
    "        output_file = os.path.join(CFG.output_dir, f\"{db_id}_{i}.json\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {i}: {e}\")\n",
    "\n",
    "# 写入汇总文件\n",
    "with open(os.path.join(CFG.output_dir, \"all_results.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"处理完成，结果保存在: {CFG.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
