{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c41d5d-fb5d-42b5-9d75-a97dd8eaa980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(model_name='/data/qwen2-7b-instruct', prompt_file='QP_prompt.txt', input_json='schema_linking_result.json', output_dir='result', max_new_tokens=1536, batch_size=4, do_sample=True, temperature=0.6, use_fp16=True, device_map='auto')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch-generates NL2SQL 结果并保存为 JSON 文件。\n",
    "\n",
    "关键优化：\n",
    "1. **一次读取模板 & format_map**避免在循环里多次 I/O 和正则替换。\n",
    "2. **批量推理**利用 pipeline 的 `batch_size` 提升吞吐。\n",
    "3. **no_grad + fp16/8bit**减小显存占用，推理更快。\n",
    "4. **路径与超参集中管理**便于脚本化 / 日后复现。\n",
    "5. **进度可视化 (tqdm)**长任务更直观。\n",
    "6. **异常兜底**避免单条数据崩溃拖垮整体。\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# ---------- 可调参数 ----------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = r\"/data/qwen2-7b-instruct\"\n",
    "    prompt_file: str = \"QP_prompt.txt\"\n",
    "    input_json: str = \"schema_linking_result.json\"\n",
    "    output_dir: str = \"result\"\n",
    "    max_new_tokens: int = 1536\n",
    "    batch_size: int = 4          # 根据显存灵活调整\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.6\n",
    "    use_fp16: bool = True        # 或用 8bit/4bit 量化\n",
    "    device_map: str = \"auto\"\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "def load_prompt_template(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def format_prompt(template: str, db_id: str, question: str,\n",
    "                  evidence: str, schema_linking: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    使用 str.format_map 直接填充占位符，保持模板可读性。\n",
    "    模板里写 {db_id}、{question}、{evidence}、{schema_linking}\n",
    "    \"\"\"\n",
    "    return template.format_map({\n",
    "        \"db_id\": db_id,\n",
    "        \"question\": question,\n",
    "        \"evidence\": evidence,\n",
    "        \"schema_linking\": json.dumps(schema_linking, ensure_ascii=False),\n",
    "    })\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(cfg: Config):\n",
    "    # 量化示例：8bit\n",
    "    quant_cfg = None\n",
    "    if not cfg.use_fp16:\n",
    "        quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\",  # 对于 text-generation 更稳妥\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if cfg.use_fp16 else torch.float32,\n",
    "        quantization_config=quant_cfg,\n",
    "        device_map=cfg.device_map,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batched(iterable: List[Any], n: int):\n",
    "    \"\"\"将列表分批切片 (Python 3.12 里有 itertools.batched)\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i : i + n]\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_sql_block(generated_text: str) -> str:\n",
    "    \"\"\"从模型输出中提取 ```sql ... ``` 中间内容\"\"\"\n",
    "    pattern = r\"```sql\\s+(.*?)\\s*```\"\n",
    "    matches = re.findall(pattern, generated_text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return matches[-1].strip()\n",
    "    return generated_text.strip()  # fallback\n",
    "    \n",
    "def generate():\n",
    "    cfg = CFG\n",
    "    root = Path(__file__).resolve().parent\n",
    "    input_path = root / cfg.input_json\n",
    "    out_dir = root / cfg.output_dir\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    data: List[Dict[str, Any]] = json.loads(Path(input_path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    prompt_tpl = load_prompt_template(root / cfg.prompt_file)\n",
    "    tokenizer, model = load_model_and_tokenizer(cfg)\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=cfg.batch_size,\n",
    "        do_sample=cfg.do_sample,\n",
    "        device_map=cfg.device_map,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # —— 主循环 ——\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(list(batched(data, cfg.batch_size)), desc=\"Generating\"):\n",
    "            prompts = [\n",
    "                format_prompt(\n",
    "                    prompt_tpl,\n",
    "                    item.get(\"db_id\", \"\"),\n",
    "                    item.get(\"question\", \"\"),\n",
    "                    item.get(\"evidence\", \"\"),\n",
    "                    item.get(\"schema_linking\", {}),\n",
    "                )\n",
    "                for item in batch\n",
    "            ]\n",
    "\n",
    "            # 生成\n",
    "            outputs = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "            # pipeline 在 batch 模式下返回 List[List[Dict]]\n",
    "            for item, gen in zip(batch, outputs):\n",
    "                text: str = gen[0][\"generated_text\"]\n",
    "                sql_key = \"sql statement:\"\n",
    "                sql_start = text.lower().find(sql_key)\n",
    "                sql = text[sql_start + len(sql_key):].strip() if sql_start != -1 else text.strip()\n",
    "\n",
    "                result = {\n",
    "                    \"db_id\": item.get(\"db_id\"),\n",
    "                    \"question\": item.get(\"question\"),\n",
    "                    \"evidence\": item.get(\"evidence\"),\n",
    "                    \"schema_linking\": item.get(\"schema_linking\"),\n",
    "                    \"sql\": sql,\n",
    "                }\n",
    "                results.append(result)\n",
    "    return results\n",
    "\n",
    "cfg = CFG\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d53f370-de68-49cd-99e1-c6c7bb693bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = Path(\"/home/yangliu26/CHASE/candidates\")\n",
    "input_path = root / cfg.input_json\n",
    "out_dir = root / cfg.output_dir\n",
    "out_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd66690-bbe3-4126-8e7f-ea294abf9849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data: List[Dict[str, Any]] = json.loads(Path(input_path).read_text(encoding=\"utf-8\"))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01306cf6-8216-4566-b82e-92da4694289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tpl = load_prompt_template(root / cfg.prompt_file)\n",
    "# print(format_prompt(\n",
    "#                 prompt_tpl,\n",
    "#                 \"test\",\n",
    "#                 \"hello, world\",\n",
    "#                 \"this is a hint\",\n",
    "#                 \"schema\",\n",
    "#             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff25724-e1c0-4262-ba25-d82105c9e1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74456ca421f14e6c81ec415d053b512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model_and_tokenizer(cfg)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=cfg.batch_size,\n",
    "    do_sample=cfg.do_sample,\n",
    "    temperature=cfg.temperature,\n",
    "    device_map=cfg.device_map,\n",
    "    return_full_text=False,     # 仅返回新增 token，不含 prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ef0e2a6-60e6-4558-ae7a-7d42a113b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in translating natural language questions into SQL queries using a Query Plan approach.\n",
      "\n",
      "Your task:\n",
      "- Given the database, question, evidence, and schema linking, generate a step-by-step Query Plan followed by the final SQL query.\n",
      "\n",
      "Rules:\n",
      "- Think step-by-step according to the standard Query Plan structure.\n",
      "- Do not skip any steps.\n",
      "- Output only what is required: the Query Plan and Final SQL.\n",
      "- Final SQL must be enclosed inside a ```sql code block.\n",
      "- Do not output any additional text, headers, or explanations beyond the required sections.\n",
      "\n",
      "Reference structure for Query Plan:\n",
      "1. Understand the intent\n",
      "2. Locate target tables and columns\n",
      "3. Identify filter conditions\n",
      "4. Determine aggregation, grouping, ordering\n",
      "5. Handle joins if needed\n",
      "6. Build subqueries if needed\n",
      "7. Formulate final SQL\n",
      "\n",
      "[Input]\n",
      "Given the following information:\n",
      "\n",
      "- **Database**: movie_platform\n",
      "- **Question**: Name the movie with the most ratings.\n",
      "- **Evidence** (schema and sample data): movie with the most rating refers to MAX(SUM(rating_score));\n",
      "- **Schema Linking Results**: {\"movies\": [\"movies\", \"movie_url\", \"movie_id\", \"movie_title\", \"movies\", \"movie_popularity\"], \"ratings\": [\"movie_id\", \"ratings\", \"rating_url\", \"ratings\", \"rating_url\", \"rating_score\", \"rating_id\"], \"lists\": [\"lists\"], \"ratings_users\": [\"ratings_users\"]}\n",
      "\n",
      "[Start Output]\n",
      "\n",
      "### Query Plan:\n",
      "(Your step-by-step reasoning...)\n",
      "\n",
      "### Final SQL:\n",
      "```sql\n",
      "(Your SQL query here)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "item = data[3]\n",
    "prompts = format_prompt(\n",
    "                prompt_tpl,\n",
    "                item.get(\"db_id\", \"\"),\n",
    "                item.get(\"question\", \"\"),\n",
    "                item.get(\"evidence\", \"\"),\n",
    "                item.get(\"schema_linking\", {}),\n",
    "            )\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ddfef90-b280-4e39-b9c8-aa3c63f0d9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \" sql\\n### Query Plan:\\n1. **Understand the intent**: The question asks for the movie title that has the highest total sum of ratings.\\n2. **Locate target tables and columns**: We need to join'movies' table with 'ratings' table to get the total ratings for each movie.\\n   - Target table:'movies', 'ratings'\\n   - Target columns:'movie_title', 'rating_score'\\n3. **Identify filter conditions**: We need to calculate the sum of ratings for each movie.\\n   - Filter condition: Join'movies' on 'ratings' using'movie_id'.\\n4. **Determine aggregation, grouping, ordering**: Aggregate the sum of ratings per movie and order by descending sum.\\n   - Aggregation: SUM(rating_score)\\n   - Grouping: GROUP BY'movie_title'\\n   - Ordering: ORDER BY SUM(rating_score) DESC\\n5. **Handle joins if needed**: Join'movies' and 'ratings' tables based on'movie_id'.\\n   - Join type: INNER JOIN\\n6. **Build subqueries if needed**: No subqueries are necessary for this query.\\n7. **Formulate final SQL**: Select the top movie title with the highest sum of ratings.\\n\\n### Final SQL:\\n```sql\\nSELECT m.movie_title\\nFROM movies m\\nINNER JOIN ratings r ON m.movie_id = r.movie_id\\nGROUP BY m.movie_title\\nORDER BY SUM(r.rating_score) DESC\\nLIMIT 1;\\n``` sql\"}]\n"
     ]
    }
   ],
   "source": [
    "output = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfcaabf8-109a-43db-8dab-fed4e28c67c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1568836-0b0e-48aa-810c-81948d8b61eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" sql\\n### Query Plan:\\n1. **Understand the intent**: The question asks for the movie title that has the highest total sum of ratings.\\n2. **Locate target tables and columns**: We need to join'movies' table with 'ratings' table to get the total ratings for each movie.\\n   - Target table:'movies', 'ratings'\\n   - Target columns:'movie_title', 'rating_score'\\n3. **Identify filter conditions**: We need to calculate the sum of ratings for each movie.\\n   - Filter condition: Join'movies' on 'ratings' using'movie_id'.\\n4. **Determine aggregation, grouping, ordering**: Aggregate the sum of ratings per movie and order by descending sum.\\n   - Aggregation: SUM(rating_score)\\n   - Grouping: GROUP BY'movie_title'\\n   - Ordering: ORDER BY SUM(rating_score) DESC\\n5. **Handle joins if needed**: Join'movies' and 'ratings' tables based on'movie_id'.\\n   - Join type: INNER JOIN\\n6. **Build subqueries if needed**: No subqueries are necessary for this query.\\n7. **Formulate final SQL**: Select the top movie title with the highest sum of ratings.\\n\\n### Final SQL:\\n```sql\\nSELECT m.movie_title\\nFROM movies m\\nINNER JOIN ratings r ON m.movie_id = r.movie_id\\nGROUP BY m.movie_title\\nORDER BY SUM(r.rating_score) DESC\\nLIMIT 1;\\n``` sql\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0875efa3-7efe-4aab-a0cf-ffd59bb4c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT m.movie_title\n",
      "FROM movies m\n",
      "INNER JOIN ratings r ON m.movie_id = r.movie_id\n",
      "GROUP BY m.movie_title\n",
      "ORDER BY SUM(r.rating_score) DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "print(extract_sql_block(output[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e50f2be-5f02-4c82-9dc5-422e818e98af",
   "metadata": {},
   "source": [
    "# —— 主循环 ——\n",
    "cnt=0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(list(batched(data, cfg.batch_size)), desc=\"Generating\"):\n",
    "        prompts = [\n",
    "            format_prompt(\n",
    "                prompt_tpl,\n",
    "                item.get(\"db_id\", \"\"),\n",
    "                item.get(\"question\", \"\"),\n",
    "                item.get(\"evidence\", \"\"),\n",
    "                item.get(\"schema_linking\", {}),\n",
    "            )\n",
    "            for item in batch\n",
    "        ]\n",
    "        # print(prompts)\n",
    "        # 生成\n",
    "        outputs = generator(prompts, max_new_tokens=cfg.max_new_tokens)\n",
    "        # pipeline 在 batch 模式下返回 List[List[Dict]]\n",
    "        for item, gen in zip(batch, outputs):\n",
    "            text: str = gen[0][\"generated_text\"]\n",
    "            sql_key = \"sql statement:\"\n",
    "            sql_start = text.lower().find(sql_key)\n",
    "            sql = text[sql_start + len(sql_key) :].strip() if sql_start != -1 else text.strip()\n",
    "\n",
    "            result = {\n",
    "                \"db_id\": item.get(\"db_id\"),\n",
    "                \"question\": item.get(\"question\"),\n",
    "                \"evidence\": item.get(\"evidence\"),\n",
    "                \"schema_linking\": item.get(\"schema_linking\"),\n",
    "                \"sql\": sql,\n",
    "            }\n",
    "\n",
    "            out_file = out_dir / f\"{item['db_id']}_{cnt}.json\"\n",
    "            cnt += 1\n",
    "            out_file.write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ All done! Results saved to: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ceb6b-0e72-4d67-b0c1-52871f02fb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
