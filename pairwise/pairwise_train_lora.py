from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
import torch
import json
import os
import evaluate

# æ ¼å¼åŒ–pairwiseç¤ºä¾‹
def format_pairwise_example(example):
    return {
        "text": example['prompt'],
        "label": 0 if example["label"] == "A" else 1
    }
    
# åŠ è½½ JSON æ•°æ®é›†
def load_json_dataset(path):
    with open(path, 'r', encoding='utf-8') as f:
        raw = json.load(f)
    return Dataset.from_list([format_pairwise_example(ex) for ex in raw])

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    accuracy = evaluate.load("accuracy")
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

# ä¸»å‡½æ•°
def main():
    # è®¾ç½®è·¯å¾„å’Œå‚æ•°
    data_path = "/home/yangliu26/CHASE/utils/pairwise_datas.json"
    model_name = "/home/yangliu26/qwen3-8b"
    output_dir = "./pairwise_selector_model/qwen3-8b-lora/"
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        r=16,                     # LoRAçŸ©é˜µçš„ç§©
        lora_alpha=32,            # LoRAçš„ç¼©æ”¾å‚æ•°
        lora_dropout=0.1,         # LoRAå±‚çš„dropoutç‡
        bias="none",              # æ˜¯å¦è®­ç»ƒåç½®é¡¹
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # éœ€è¦åº”ç”¨LoRAçš„æ¨¡å—
    )
    
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    # åŠ è½½å’Œåˆ†å‰²æ•°æ®é›†
    dataset = load_json_dataset(data_path)
    dataset = dataset.train_test_split(test_size=0.1)
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    base_model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=2,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
    
    # åº”ç”¨LoRAé…ç½®
    print("æ­£åœ¨åº”ç”¨LoRAé…ç½®...")
    model = get_peft_model(base_model, lora_config)
    model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹
    
    # ç¼–ç å‡½æ•°
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    
    print("æ­£åœ¨å¤„ç†æ•°æ®é›†...")
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
    
    print("è®­ç»ƒé›†å¤§å°:", len(tokenized_dataset["train"]))
    print("æµ‹è¯•é›†å¤§å°:", len(tokenized_dataset["test"]))
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not torch.distributed.is_initialized():
        try:
            import deepspeed
            deepspeed.init_distributed()
            print(f"âœ… æ‰‹åŠ¨åˆå§‹åŒ–å®Œæˆ: Rank {torch.distributed.get_rank()}")
        except Exception as e:
            print(f"âŒ DeepSpeed æ‰‹åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    print("æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå‚æ•°...")
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        per_device_train_batch_size=4,  # ä½¿ç”¨LoRAå¯ä»¥å¢åŠ æ‰¹é‡å¤§å°
        per_device_eval_batch_size=4,
        num_train_epochs=5,
        bf16=True,
        logging_dir="./logs",
        logging_steps=10,
        learning_rate=5e-5,  # LoRAé€šå¸¸å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        ddp_find_unused_parameters=False,
        deepspeed="/home/yangliu26/CHASE/pairwise/ds_config_lora.json",  # å¯é€‰ï¼šä¸ºLoRAä¼˜åŒ–çš„DeepSpeedé…ç½®
        gradient_accumulation_steps=2,  # æ¢¯åº¦ç´¯ç§¯
        warmup_ratio=0.1,  # é¢„çƒ­æ¯”ä¾‹
    )
    
    print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– Trainer...")
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )
    
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    print("âœ… è®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model(output_dir)
    
    # è¯„ä¼°æ¨¡å‹
    print("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
    eval_results = trainer.evaluate()
    print(f"è¯„ä¼°ç»“æœ: {eval_results}")
    
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")

if __name__ == "__main__":
    main()