{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b119528-3fd4-4e21-98a3-1b7ce8231570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from schema_parser import load_schema\n",
    "from schema_linker import SchemaLinker\n",
    "from async_keyword_extractor import KeywordExtractor\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "TRAIN_JSON_PATH = r\"/home/yangliu26/data/train/train.json\"\n",
    "SCHEMA_JSON_PATH = r\"/home/yangliu26/data/train/train_tables.json\"\n",
    "MODEL_PATH = r\"/home/yangliu26/qwen3-8b\"\n",
    "\n",
    "# 蜉霓ｽschema菫｡諱ｯ\n",
    "def get_schema_map(schema_json_path: str) -> Dict[str, Any]:\n",
    "    schema = load_schema(schema_json_path)\n",
    "    db_schema_map = {}\n",
    "    for db in schema:\n",
    "        db_id = db[\"db_id\"] if isinstance(db, dict) and \"db_id\" in db else db.get(\"db_id\", \"\")\n",
    "        db_schema_map[db_id] = db\n",
    "    return db_schema_map\n",
    "\n",
    "def link_keywords_to_schema(keywords, schema_info):\n",
    "    # 邂蜊不chema linking騾ｻ霎托ｼ壼ｳ髞ｮ隸堺ｸ手｡ｨ蜷阪∝ｭ玲ｮｵ蜷榊★讓｡邉雁源驟構n",
    "    linked = []\n",
    "    tables = schema_info.get(\"table_names_original\", [])\n",
    "    columns = [col[1] for col in schema_info.get(\"column_names_original\", [])]\n",
    "    for kw in keywords:\n",
    "        for t in tables:\n",
    "            if kw.lower() in t.lower():\n",
    "                linked.append((kw, \"table\", t))\n",
    "        for c in columns:\n",
    "            if kw.lower() in c.lower():\n",
    "                linked.append((kw, \"column\", c))\n",
    "    return linked\n",
    "\n",
    "async def async_main():\n",
    "    with open(TRAIN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # 蜿門燕10荳ｪ謨ｰ謐ｮ菴應ｸｺ豬玖ｯ表n",
    "    data = data[:10]\n",
    "    schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "    extractor = KeywordExtractor(MODEL_PATH)\n",
    "    linker = SchemaLinker()\n",
    "    # 謠仙叙蜃ｺ謇譛蛾琉鬚禄n",
    "    questions = [sample[\"question\"] for sample in data]\n",
    "    # 謠仙叙蜃ｺ豈丈ｸｪ髣ｮ鬚倡噪蜈ｳ髞ｮ隸構n",
    "    # all_keywords = await extractor.batch_extract(questions)\n",
    "    print(\"剥 Extracting keywords 窶ｦ\")\n",
    "    all_keywords = await extractor.batch_extract(\n",
    "        tqdm(questions, desc=\"Keyword-extract\", unit=\"q\")\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for sample, keywords in zip(data, all_keywords):\n",
    "        db_id = sample[\"db_id\"]\n",
    "        question = sample[\"question\"]\n",
    "        evidence = sample[\"evidence\"]\n",
    "        schema_info = schema_map.get(db_id, {})\n",
    "        # schema-linking\n",
    "        linker.build_index(schema_info)\n",
    "        linking_results = linker.search(keywords)\n",
    "        # 譬ｼ蠑丞喧\n",
    "        formatted_linking = {}\n",
    "        for matches in linking_results:\n",
    "            for kw, schema_item, table_name, score in matches:\n",
    "                if table_name not in formatted_linking:\n",
    "                    formatted_linking[table_name] = []\n",
    "                formatted_linking[table_name].append(schema_item)\n",
    "\n",
    "        results.append({\n",
    "            \"db_id\": db_id,\n",
    "            \"question\": question,\n",
    "            \"evidence\": evidence,\n",
    "            \"keywords\": keywords,\n",
    "            \"schema_linking\": formatted_linking\n",
    "        })\n",
    "        \n",
    "    # 霎灘ｺ扈捺棡蛻ｰ蠖灘燕譁莉ｶ逶ｮ蠖穂ｸ狗噪schema_linking_result.json\n",
    "    out_path = os.path.join(os.path.dirname(__file__), \"schema_linking_result.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Schema linking扈捺棡蟾ｲ菫晏ｭ伜芦: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb7b7b7-cef2-4f83-82a8-75ded93965fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'db_id': 'movie_platform', 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.', 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;', 'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1'}]\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bef4e22-1366-4eb0-a863-6e93d43d7550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# 蜷謨ｰ謐ｮ蠎灘慍schema\n",
    "schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "print(schema_map.__class__)\n",
    "# 謇灘魂隨ｬ荳荳ｪ髞ｮ蛟ｼ蟇ｹ\n",
    "# first_key = next(iter(schema_map))\n",
    "# print(first_key, schema_map[first_key])\n",
    "# print(json.dumps(schema_map[first_key], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce485c0-48b1-4d69-8d79-c0f44dd118a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"db_id\": \"movie_platform\",\n",
      "    \"question\": \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\",\n",
      "    \"evidence\": \"released in the year 1945 refers to movie_release_year = 1945;\",\n",
      "    \"SQL\": \"SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "data = data[:1]\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439d606f-f25f-48a2-8db6-27923409f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb334f9ca46438b9f7901c01ed88df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parsing Failed] Content: ```json\n",
      "{\n",
      "  \"keywords\": [\"movie titles\", \"released\", \"1945\", \"sort\", \"descending\", \"popularity\"]\n",
      "}\n",
      "``` \n",
      "\n",
      "Okay, let's tackle this query. The user is asking for movie titles released in 1945 and wants them sorted by popularity in descending order. First, I need to extract the key entities and attributes.\n",
      "\n",
      "The main entities here are \"movie titles\" since that's what they're asking for. The year 1945 is a crucial filter, so that's definitely a keyword. The action of releasing is important too, so \"released\" should be included. \n",
      "\n",
      "Then there's the sorting part. The user mentioned sorting by popularity, so \"popularity\" is a key attribute. The order is descending, so \"descending\" needs to be in the list. \n",
      "\n",
      "Wait, should \"sort\" be included? The instruction says to extract essential elements. Since the user is asking to sort the listing, \"sort\" is part of the action. But maybe it's redundant because the sorting is implied by the mention of popularity and descending. Hmm, but the original example included \"sort\" as a keyword. Let me check the example again. \n",
      "\n",
      "In the example given, the input\n",
      "[Error Info] Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "extractor = KeywordExtractor(MODEL_PATH)\n",
    "linker = SchemaLinker()\n",
    "# 謠仙叙蜃ｺ謇譛蛾琉鬚禄n",
    "questions = [sample[\"question\"] for sample in data]\n",
    "# 謠仙叙蜃ｺ豈丈ｸｪ髣ｮ鬚倡噪蜈ｳ髞ｮ隸構n",
    "all_keywords = await extractor.batch_extract(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34957c4b-59fb-4c6c-8311-8c958c576a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f39726-99ef-42be-9c2c-46be79b7f241",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m schema_info \u001b[38;5;241m=\u001b[39m schema_map\u001b[38;5;241m.\u001b[39mget(db_id, {})\n\u001b[0;32m----> 5\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m(question)\n\u001b[1;32m      6\u001b[0m linking \u001b[38;5;241m=\u001b[39m link_keywords_to_schema(keywords, schema_info)\n\u001b[1;32m      7\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: db_id,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_linking\u001b[39m\u001b[38;5;124m\"\u001b[39m: linking\n\u001b[1;32m     12\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "for sample in data:\n",
    "    db_id = sample[\"db_id\"]\n",
    "    question = sample[\"question\"]\n",
    "    schema_info = schema_map.get(db_id, {})\n",
    "    keywords = keyword(question)\n",
    "    linking = link_keywords_to_schema(keywords, schema_info)\n",
    "    results.append({\n",
    "        \"db_id\": db_id,\n",
    "        \"question\": question,\n",
    "        \"keywords\": keywords,\n",
    "        \"schema_linking\": linking\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cb9f0-c582-4495-a5e9-9aaa30489437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 霎灘ｺ扈捺棡\n",
    "    out_path = os.path.join(os.path.dirname(__file__), \"schema_linking_result.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Schema linking扈捺棡蟾ｲ菫晏ｭ伜芦: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2fc95-2580-42e0-8cc1-25153505aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
