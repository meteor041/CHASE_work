{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b119528-3fd4-4e21-98a3-1b7ce8231570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from schema_parser import load_schema\n",
    "from schema_linker import SchemaLinker\n",
    "from async_keyword_extractor import KeywordExtractor\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "TRAIN_JSON_PATH = r\"/home/yangliu26/data/train/train.json\"\n",
    "SCHEMA_JSON_PATH = r\"/home/yangliu26/data/train/train_tables.json\"\n",
    "MODEL_PATH = r\"/home/yangliu26/qwen3-8b\"\n",
    "\n",
    "# 加载schema信息\n",
    "def get_schema_map(schema_json_path: str) -> Dict[str, Any]:\n",
    "    schema = load_schema(schema_json_path)\n",
    "    db_schema_map = {}\n",
    "    for db in schema:\n",
    "        db_id = db[\"db_id\"] if isinstance(db, dict) and \"db_id\" in db else db.get(\"db_id\", \"\")\n",
    "        db_schema_map[db_id] = db\n",
    "    return db_schema_map\n",
    "\n",
    "def link_keywords_to_schema(keywords, schema_info):\n",
    "    # 简单schema linking逻辑：关键词与表名、字段名做模糊匹配\n",
    "    linked = []\n",
    "    tables = schema_info.get(\"table_names_original\", [])\n",
    "    columns = [col[1] for col in schema_info.get(\"column_names_original\", [])]\n",
    "    for kw in keywords:\n",
    "        for t in tables:\n",
    "            if kw.lower() in t.lower():\n",
    "                linked.append((kw, \"table\", t))\n",
    "        for c in columns:\n",
    "            if kw.lower() in c.lower():\n",
    "                linked.append((kw, \"column\", c))\n",
    "    return linked\n",
    "\n",
    "async def async_main():\n",
    "    with open(TRAIN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # 取前10个数据作为测试\n",
    "    data = data[:10]\n",
    "    schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "    extractor = KeywordExtractor(MODEL_PATH)\n",
    "    linker = SchemaLinker()\n",
    "    # 提取出所有问题\n",
    "    questions = [sample[\"question\"] for sample in data]\n",
    "    # 提取出每个问题的关键词\n",
    "    # all_keywords = await extractor.batch_extract(questions)\n",
    "    print(\"🔍 Extracting keywords …\")\n",
    "    all_keywords = await extractor.batch_extract(\n",
    "        tqdm(questions, desc=\"Keyword-extract\", unit=\"q\")\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for sample, keywords in zip(data, all_keywords):\n",
    "        db_id = sample[\"db_id\"]\n",
    "        question = sample[\"question\"]\n",
    "        evidence = sample[\"evidence\"]\n",
    "        schema_info = schema_map.get(db_id, {})\n",
    "        # schema-linking\n",
    "        linker.build_index(schema_info)\n",
    "        linking_results = linker.search(keywords)\n",
    "        # 格式化\n",
    "        formatted_linking = {}\n",
    "        for matches in linking_results:\n",
    "            for kw, schema_item, table_name, score in matches:\n",
    "                if table_name not in formatted_linking:\n",
    "                    formatted_linking[table_name] = []\n",
    "                formatted_linking[table_name].append(schema_item)\n",
    "\n",
    "        results.append({\n",
    "            \"db_id\": db_id,\n",
    "            \"question\": question,\n",
    "            \"evidence\": evidence,\n",
    "            \"keywords\": keywords,\n",
    "            \"schema_linking\": formatted_linking\n",
    "        })\n",
    "        \n",
    "    # 输出结果到当前文件目录下的schema_linking_result.json\n",
    "    out_path = os.path.join(os.path.dirname(__file__), \"schema_linking_result.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Schema linking结果已保存到: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb7b7b7-cef2-4f83-82a8-75ded93965fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'db_id': 'movie_platform', 'question': 'Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.', 'evidence': 'released in the year 1945 refers to movie_release_year = 1945;', 'SQL': 'SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1'}]\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bef4e22-1366-4eb0-a863-6e93d43d7550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# 各数据库地schema\n",
    "schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "print(schema_map.__class__)\n",
    "# 打印第一个键值对\n",
    "# first_key = next(iter(schema_map))\n",
    "# print(first_key, schema_map[first_key])\n",
    "# print(json.dumps(schema_map[first_key], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce485c0-48b1-4d69-8d79-c0f44dd118a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"db_id\": \"movie_platform\",\n",
      "    \"question\": \"Name movie titles released in year 1945. Sort the listing by the descending order of movie popularity.\",\n",
      "    \"evidence\": \"released in the year 1945 refers to movie_release_year = 1945;\",\n",
      "    \"SQL\": \"SELECT movie_title FROM movies WHERE movie_release_year = 1945 ORDER BY movie_popularity DESC LIMIT 1\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "data = data[:1]\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439d606f-f25f-48a2-8db6-27923409f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb334f9ca46438b9f7901c01ed88df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/envd/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parsing Failed] Content: ```json\n",
      "{\n",
      "  \"keywords\": [\"movie titles\", \"released\", \"1945\", \"sort\", \"descending\", \"popularity\"]\n",
      "}\n",
      "``` \n",
      "\n",
      "Okay, let's tackle this query. The user is asking for movie titles released in 1945 and wants them sorted by popularity in descending order. First, I need to extract the key entities and attributes.\n",
      "\n",
      "The main entities here are \"movie titles\" since that's what they're asking for. The year 1945 is a crucial filter, so that's definitely a keyword. The action of releasing is important too, so \"released\" should be included. \n",
      "\n",
      "Then there's the sorting part. The user mentioned sorting by popularity, so \"popularity\" is a key attribute. The order is descending, so \"descending\" needs to be in the list. \n",
      "\n",
      "Wait, should \"sort\" be included? The instruction says to extract essential elements. Since the user is asking to sort the listing, \"sort\" is part of the action. But maybe it's redundant because the sorting is implied by the mention of popularity and descending. Hmm, but the original example included \"sort\" as a keyword. Let me check the example again. \n",
      "\n",
      "In the example given, the input\n",
      "[Error Info] Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "schema_map = get_schema_map(SCHEMA_JSON_PATH)\n",
    "extractor = KeywordExtractor(MODEL_PATH)\n",
    "linker = SchemaLinker()\n",
    "# 提取出所有问题\n",
    "questions = [sample[\"question\"] for sample in data]\n",
    "# 提取出每个问题的关键词\n",
    "all_keywords = await extractor.batch_extract(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34957c4b-59fb-4c6c-8311-8c958c576a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f39726-99ef-42be-9c2c-46be79b7f241",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m schema_info \u001b[38;5;241m=\u001b[39m schema_map\u001b[38;5;241m.\u001b[39mget(db_id, {})\n\u001b[0;32m----> 5\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m(question)\n\u001b[1;32m      6\u001b[0m linking \u001b[38;5;241m=\u001b[39m link_keywords_to_schema(keywords, schema_info)\n\u001b[1;32m      7\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: db_id,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: keywords,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_linking\u001b[39m\u001b[38;5;124m\"\u001b[39m: linking\n\u001b[1;32m     12\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "for sample in data:\n",
    "    db_id = sample[\"db_id\"]\n",
    "    question = sample[\"question\"]\n",
    "    schema_info = schema_map.get(db_id, {})\n",
    "    keywords = keyword(question)\n",
    "    linking = link_keywords_to_schema(keywords, schema_info)\n",
    "    results.append({\n",
    "        \"db_id\": db_id,\n",
    "        \"question\": question,\n",
    "        \"keywords\": keywords,\n",
    "        \"schema_linking\": linking\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cb9f0-c582-4495-a5e9-9aaa30489437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "    out_path = os.path.join(os.path.dirname(__file__), \"schema_linking_result.json\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Schema linking结果已保存到: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2fc95-2580-42e0-8cc1-25153505aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
